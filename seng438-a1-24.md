>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 24      |
|-----------------|
| Ahad Ali                |   
| Mushtaba Al Yasseen               |   
| Parbir Lehal               |   
| Athul Rajagopal                |   


**Table of Contents**


[1 Introduction](#introduction)

[2 High-level description of the exploratory testing plan](#high-level-description-of-the-exploratory-testing-plan)

[3 Comparison of exploratory and manual functional testing](#comparison-of-exploratory-and-manual-functional-testing)

[4 Notes and discussion of the peer reviews of defect reports](#notes-and-discussion-of-the-peer-reviews-of-defect-reports)

[5 How the pair testing was managed, and how teamwork, effort was
divided](#how-the-pair-testing-was-managed-and-how-teamwork-effort-was-divided)

[6 Difficulties encountered, challenges overcome, and lessons
learned](#difficulties-encountered-challenges-overcome-and-lessons-learned)

[7 Comments and feedback on the lab and lab document itself](#comments-and-feedback-on-the-lab-and-lab-document-itself)

# Introduction

The objective of this lab is to obtain hands-on experience with testing methodologies in a software system, including exploratory testing, manual scripted testing, and regression testing. Prior to the lab, we were introduced to these different kinds of testing procedures, and the rationale behind them. Exploratory testing is an informal testing method where the tester freely explores the application to find defects without adhering to a specific test script or strategy. Conversely, manual scripted testing involves the use a specific test script that is created in advance, which the tester must stick to. Lastly, regression testing is a type of testing that is used to ensure that changes to the application or system do not introduce new defects or break existing functionality. This type of testing is typically done after changes have been made to the application or system, and is used to verify that the changes did not negatively impact the rest of the application or system. Using a combination of these methodologies, we were able to visualize the distinct purposes of each, and further understand their unique advantages and disadvantages.

# High-level description of the exploratory testing plan

Mushtaba & Parbir:

* We intend to test the system by simulating how the ATM machine would react to an individual using the system who could input correct and or incorrect inputs. The unfamiliar and unpredictable nature of our test cases are intended to also simulate how an individual could use the system in the real world, which makes sure that we single out all unintended behaviours/outputs. For instance, our test cases will include both common and likely paths that individuals would take while using the system, such as inserting their card, entering their pin, and depositing/withdrawing cash. However, our test cases also will also include some exceptional paths that could be intentionally/accidentally taken, such as pressing buttons that should result in no action, attempting to withdraw cash with no money in the ATM, or continually entering incorrect pin numbers. In addition, we will also test whether or not the ATM system retains the correct balances after withdrawing/depositing from and into accounts.

Ahad & Athul: 

* Our exploratory testing plan included testing the ATM functionality in Appendix B (High Level Requirements). The first test involved testing all multiples of $20 withdrawals from the menu. We ensured each and every option in the menu worked, was a multiple of $20.00, and correctly withdrew the correct amount when looking at the receipt. The next overarching test involved ensuring a customer was able to make a deposit into the account and again ensuring the ATM system correctly deposited the correct amount when looking at the receipt. In the test, we also ensured the customer was able to accept and view the envelope. The third test was ensuring that a transfer of money between any two accounts linked to the card could be made. This meant we ensured that a user can, for example, transfer money from their savings to checking account if both accounts exist and the savings account in this case had sufficient funds. The fourth test we conducted was making a balance inquiry of any account linked to the card, which meant to ensure all accounts available to the user accurately reflected the correct balance. Our fifth and final test ensured we were able to abort a transaction in progress by pressing the Cancel key rather than proceeding with the transaction.


# Comparison of exploratory and manual functional testing

Exploratory testing (ET) allows the tester to quickly familiarize themselves with the System Under Test (SUT). As the tester goes through the SUT, they start to gain a greater understanding than they would if they just followed a test suite. A new tester may struggle with exploratory testing because they aren’t primed to notice the small bugs and defects that an experienced tester would be able to seek out. Consequently, skill is put into question when testing, therefore having pairs to perform exploratory testing can greatly increase the efficiency of this style of testing. Exploratory testing plays more towards the creative side of testing, as the testers are given complete freedom to uncover bugs. The benefit this provides is testers may uncover bugs that don’t exist in the testing suite. It is also important to note that once a bug is found, it gives testers free rein to find if a similar bug exists within the SUT; this would not be possible if a tester is strictly doing Manual Scripted testing (MFT). The downsides of ET is the lack of efficiency, going into a SUT blindly can take a lot of time and resources to complete. There is no known end goal or finishline, so its difficult to know if a tester has covered all of the SUT or is missing existing bugs. Lastly, as previously mentioned, the skill it requires can exhaust more resources, such as another tester or an experienced tester to accurately perform this type of testing. 

Manual Functional Testing is a type of testing where a given outline (test suite) is used to drive testing on the SUT. The test suite is a clear guide that includes a use case, function being tested, initial system state, input, and expected output. This allows a tester to accurately replicate a given input and determine if the output is consistent with the expected output, and if it doesn’t a bug can easily be distinguished and outlined. This pattern allows testers to be very efficient, they know what to test, how to test it and what is expected, therefore, alleviating the skill it’d require in ET. Manual Functional Testing also tests usually the most important and key functionalities. The effectiveness can be very useful in time limited circumstances. Inevitably, certain bugs and defects may be missed if a comprehensive testing suite isn’t carefully crafted. This is the tradeoff that exists for an efficient testing style, efficient but not effective. Whereas exploratory testing is effective but not very efficient. Using both styles of testing can allow for a complete bugless SUT, if done correctly, and therefore both styles play a crucial role in real world software development cycles. 


# Notes and discussion of the peer reviews of defect reports

After reviewing the defect reports completed by both pairs in their entirety, we quickly recognized some common trends within the collection. First and foremost, we found several more defects through exploratory testing rather than manual scripted testing. While this may initially seem to be a positive in the context of finding as many faults with the SUT as possible, we came across another revelation. Despite there being a higher yield of defect reports through exploratory testing, it was more of a hassle to track these reports through regression testing, as opposed to those that derived from manual scripted testing. We believe this is mainly due to the fact that, when using a scripted approach, each test is well documented and organized by relevant groupings. This in turn means that future versions of the SUT can be set against consistent benchmarks, and so any observed improvements are clearer to report. On the other hand, regression testing on defects reported through exploratory testing felt just as stochastic as before, even if the issue was fixed in Version 1.1. The implications of this are that exploratory testing may be able to find more faults with the program on average, but this comes at the cost of benchmarking and documentation, which we believe is more useful in tracking the progress of development in the long-term.

# How the pair testing was managed, and how teamwork, effort was divided

Our team used Discord as the primary communication medium for Assignment 1. Therefore, through scheduled discord meetings the lab work was split between the two pairs.

The two pairs for the purpose of this lab were:
* Ahad and Athul (Pair 1)
* Mushtaba and Parbir (Pair 2)

For exploratory testing, Partner 1 (Ahad and Mushtaba) acted as the tester and shared their screen with version 1.0 of the ATM system. Meanwhile, Partner 2 (Athul and Parbir) acted as the developer and created issues or bug reports in the Jira application. Partner 1 and Partner 2 both shared their screen during virtual meetings to ensure efficiency and accuracy during the exploratory testing purpose. For Pair 1, both members came up with the exploratory testing plan together based on the requirements outlined in Appendix B, while for Pair 2, Partner 1 (Mushtaba) created the exploratory testing plan.

After both pairs completed exploratory testing, for manual scripted testing, Pair 1 took up the first 20 test cases from Appendix C, while Pair 2 took up the next 20 test cases from Appendix C. This time Partner 2 acted as the developer while Partner 1 acted as the tester. This ensured all members of team had a chance to act as both tester and developer. During this phase of testing, if a bug was found in the test case but was already found in exploratory testing, the bug report would be updated to notify developers or viewers that the bug was found in both phases of testing. If the bug was not previously reported, Partner 1 would instead create a new bug report.

Finally, for regression testing, both pairs ensured to test all the bug reports in the exploratory testing phase and test all 20 test cases from the manual scripted testing phase again but for Version 1.1 of the ATM system. When testing the bug reports from the exploratory testing phase, Partner 1 assumed the role of tester and Parter 2 assumed the role of developer as was the case with version 1.0. For manual scripted testing, the roles were reversed, as was the case with version 1.0. If a bug persisted or was only partially resolved in this version, the status changed from TODO to IN PROGRESS and the description was also updated. In addition, the "Affects Version" field was also updated. However, if the bug was resolved, then the status changed to DONE and the "Fix Version" field was updated. Finally, if a new bug was found, then a new bug report was created.

# Difficulties encountered, challenges overcome, and lessons learned

Naturally, while coming up with an exploratory test plan, and creating test cases based on our plans, we experienced a lot of overlap/redundancy in the bug reports between the two sets of partners. For instance, Mushtaba & Parbir’s exploratory test plan was intended to simulate how an individual would interact with the ATM system, taking both expected and unexpected paths to simulate the unpredictable nature of the customer. However, Ahad & Athul created their test plan to test the high level requirements of the ATM system and thus, both exploratory plans interfered when it came to common bugs with the withdrawal, deposit, and transfer functionalities. With the use of Jira, a bug tracking/reporting system, we were able to overcome this issue as we made sure to use a common template for our bug reports, which made it really easy to identify and keep track of each issue. Thus, we were able to successfully single out all duplicate/redundant bug reports due to the early organization of a common framework that was put in place. Furthermore, what we learned the most while testing the application and creating bug reports was the significant amount of time that was spent doing so. Since we were effectively testing each functionality manually, the process was definitely a tedious procedure, especially when it came to doing three different types of testing, exploratory, MFT, and regression, something which could have been done more efficiently through the use of an automated testing system.

# Comments and feedback on the lab and lab document itself

Overall, we felt as if the lab had completed its task of making sure that students understand the applications of exploratory testing, MFT, and regression testing, and how the process can be tedious at times. Likewise, the use of industry standard software tools such as Jira or DevOps was definitely a nice touch, as it also allows us to experience how organizations would actually deal and interact with the bugs that they found through testing, and how to create proper bug reports. However, there are some little things that were confusing while reading through the instructions such as in the regression testing section where it asks to execute steps 4 and 5 in MFT, but the MFT section only has two steps. In addition, testing instructions were somewhat vague, as the document initially proposed the benefits of partner testing, but did not require anything in specific when it came to MFT and regression testing, summing it up as “a group effort”. Other than some minor grammar errors throughout the document itself, it was formatted well overall.
